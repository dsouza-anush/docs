---
title: "Embeddings & RAG playbooks"
description: "Build search and retrieval workflows on Heroku AI with OpenAI-compatible embeddings and chat completions."
fullWidth: true
---

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 3rem', padding: '0 min(8vw, 96px)' }}>
  <p>
    Heroku AI exposes OpenAI-compatible embedding models, including <code>cohere-embed-multilingual</code>. These notes adapt the OpenAI Cookbook's retrieval patterns to Heroku's stack so you can stand up semantic search, question answering, and summarization pipelines quickly. Primary references: <a href="https://cookbook.openai.com/examples/vector_databases/search_summarize" target="_blank" rel="noopener noreferrer">Search + Summarize</a> and <a href="https://cookbook.openai.com/examples/vector_databases/faq_bot" target="_blank" rel="noopener noreferrer">FAQ bot with embeddings</a>.
  </p>
</section>

## Design the retrieval flow

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 2rem', padding: '0 min(8vw, 96px)' }}>
  <div style={{
    padding: '2rem',
    borderRadius: '16px',
    background: 'linear-gradient(135deg, #ffb3d9 0%, #ffcba4 50%, #fff5d6 100%)',
    border: '1px solid rgba(255,255,255,0.2)',
    color: '#ffffff',
    marginBottom: '2rem',
    boxShadow: '0 8px 16px rgba(0, 0, 0, 0.1)',
    textShadow: '0 1px 2px rgba(0, 0, 0, 0.1)'
  }}>
    <h3 style={{ marginTop: 0, color: '#ffffff', fontWeight: '700' }}>Core pipeline</h3>
    <ol style={{ color: '#ffffff', lineHeight: '1.7' }}>
      <li>Chunk source documents (300â€“500 word windows with overlap).</li>
      <li>Embed chunks with <code>cohere-embed-multilingual</code> via the Embeddings API.</li>
      <li>Persist vectors in Postgres + pgvector or another ANN store.</li>
      <li>At query time, embed the question, retrieve top-k matches, and assemble a prompt.</li>
      <li>Send prompt + context to <code>claude-3-5-sonnet</code> or <code>claude-3-5-haiku</code> via the Chat Completions API.</li>
    </ol>
  </div>
</section>

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 3rem', padding: '0 min(8vw, 96px)' }}>

```python
from openai import OpenAI
import os

client = OpenAI(
    base_url="https://us.inference.heroku.com/v1",
    api_key=os.environ["INFERENCE_KEY"],
)

def embed_texts(texts: list[str]) -> list[list[float]]:
    result = client.embeddings.create(
        model="cohere-embed-multilingual",
        input=texts,
    )
    return [item.embedding for item in result.data]
```

<Note>
Batch up to 96 strings per request for cost efficiency, mirroring the cookbook guidance. Store the resulting vectors with their source metadata.
</Note>

</section>

## Assemble the answering prompt

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 2rem', padding: '0 min(8vw, 96px)' }}>
  <div style={{
    padding: '2rem',
    borderRadius: '16px',
    background: 'linear-gradient(135deg, #f5a3d7 0%, #ffc4e8 50%, #ffe5f5 100%)',
    border: '1px solid rgba(255,255,255,0.2)',
    color: '#ffffff',
    marginBottom: '2rem',
    boxShadow: '0 8px 16px rgba(0, 0, 0, 0.1)',
    textShadow: '0 1px 2px rgba(0, 0, 0, 0.1)'
  }}>
    <h3 style={{ marginTop: 0, color: '#ffffff', fontWeight: '700' }}>Prompt template</h3>
    <ul style={{ color: '#ffffff', lineHeight: '1.7' }}>
      <li>System message: define role, tone, and citation requirements.</li>
      <li>Human message: include the user question and formatted context snippets.</li>
      <li>Add explicit output rules (e.g., "If facts are missing, say you don't know.").</li>
    </ul>
    <p style={{ marginBottom: 0, color: '#ffffff' }}>
      Follow the cookbook's recommendation to annotate each chunk with identifiers so you can cite sources back to the end user.
    </p>
  </div>
</section>

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 3rem', padding: '0 min(8vw, 96px)' }}>

```python
def answer_with_context(question: str, context_blocks: list[str]) -> str:
    prompt = "\n\n".join(
        [f"[{i}] {block}" for i, block in enumerate(context_blocks, start=1)]
    )
    response = client.chat.completions.create(
        model="claude-3-5-sonnet",
        messages=[
            {
                "role": "system",
                "content": "You are a concise support assistant. Cite sources like [1], [2].",
            },
            {
                "role": "user",
                "content": f"Question: {question}\n\nContext:\n{prompt}",
            },
        ],
        temperature=0.2,
        max_tokens=800,
    )
    return response.choices[0].message.content
```

<Note>
Use Haiku when you prioritize throughput; upgrade to Sonnet for harder questions or multilingual outputs.
</Note>

</section>

## Operational guidance

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 2rem', padding: '0 min(8vw, 96px)' }}>
  <div style={{
    padding: '2rem',
    borderRadius: '16px',
    background: 'linear-gradient(135deg, #d4e4ff 0%, #b8d0ff 50%, #9fc5ff 100%)',
    border: '1px solid rgba(255,255,255,0.2)',
    color: '#ffffff',
    marginBottom: '2rem',
    boxShadow: '0 8px 16px rgba(0, 0, 0, 0.1)',
    textShadow: '0 1px 2px rgba(0, 0, 0, 0.1)'
  }}>
    <h3 style={{ marginTop: 0, color: '#ffffff', fontWeight: '700' }}>Best practices</h3>
    <ul style={{ color: '#ffffff', lineHeight: '1.7' }}>
      <li>Normalize text (lowercase, trim whitespace) before embedding.</li>
      <li>Refresh embeddings when source data changes; track versions in Postgres.</li>
      <li>Add automated evaluations (see our prompt patterns cookbook) to spot drift.</li>
      <li>Cache retrieval results for popular questions to reduce load.</li>
    </ul>
  </div>
</section>

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 3rem', padding: '0 min(8vw, 96px)' }}>

### Deployment on Heroku

- Run the embedding pipeline as a worker dyno; trigger rebuilds via Scheduler or webhooks.
- Store secrets (API keys, database URLs) in Config Vars, never in source.
- Use the `pgvector` extension on Heroku Postgres for similarity search.
- Stream chat responses to the client for faster perceived latency.

</section>
