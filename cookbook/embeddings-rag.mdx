---
title: "Embeddings & RAG playbooks"
description: "Build search and retrieval workflows on Heroku AI with OpenAI-compatible embeddings and chat completions."
fullWidth: true
---

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 3rem', padding: '0 min(8vw, 96px)' }}>
  <p>
    Heroku AI exposes OpenAI-compatible embedding models, including <code>cohere-embed-multilingual</code>. These notes adapt the OpenAI Cookbook’s retrieval patterns to Heroku’s stack so you can stand up semantic search, question answering, and summarization pipelines quickly. Primary references: <a href="https://cookbook.openai.com/examples/vector_databases/search_summarize" target="_blank" rel="noopener noreferrer">Search + Summarize</a> and <a href="https://cookbook.openai.com/examples/vector_databases/faq_bot" target="_blank" rel="noopener noreferrer">FAQ bot with embeddings</a>.
  </p>
</section>

## Design the retrieval flow

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 2rem', padding: '0 min(8vw, 96px)' }}>
  <div style={{
    padding: '2rem',
    borderRadius: '16px',
    background: 'linear-gradient(135deg, #ff6b9d 0%, #ff9999 20%, #ffcc99 40%, #ffee99 60%, #e6d9ff 80%, #c5d9ff 100%)',
    border: '1px solid rgba(255,255,255,0.12)',
    color: '#1a1a2e',
    marginBottom: '2rem'
  }}>
    <h3 style={{ marginTop: 0 }}>Core pipeline</h3>
    <ol>
      <li>Chunk source documents (300–500 word windows with overlap).</li>
      <li>Embed chunks with <code>cohere-embed-multilingual</code> using the <code>search_document</code> input type.</li>
      <li>Persist vectors in Postgres + pgvector or another ANN store.</li>
      <li>At query time, embed the question (<code>search_query</code> type), retrieve top-k matches, and assemble a prompt.</li>
      <li>Send prompt + context to <code>claude-3-5-sonnet</code> or <code>claude-3-5-haiku</code> via the Chat Completions API.</li>
    </ol>
  </div>
</section>

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 3rem', padding: '0 min(8vw, 96px)' }}>

```python
from openai import OpenAI
import os

client = OpenAI(
    base_url="https://us.inference.heroku.com/v1",
    api_key=os.environ["INFERENCE_KEY"],
)

def embed_texts(texts: list[str], input_type: str) -> list[list[float]]:
    result = client.embeddings.create(
        model="cohere-embed-multilingual",
        input=texts,
        encoding_format="float",
        input_type=input_type,
    )
    return [item.embedding for item in result.data]
```

<Note>
Batch up to 96 strings per request for cost efficiency, mirroring the cookbook guidance. Store the resulting vectors with their source metadata.
</Note>

</section>

## Assemble the answering prompt

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 2rem', padding: '0 min(8vw, 96px)' }}>
  <div style={{
    padding: '2rem',
    borderRadius: '16px',
    background: 'linear-gradient(180deg, #ff66cc 0%, #ff99cc 25%, #ff99bb 50%, #ff9999 75%, #ffaaaa 100%)',
    border: '1px solid rgba(255,255,255,0.12)',
    color: '#1a1a2e',
    marginBottom: '2rem'
  }}>
    <h3 style={{ marginTop: 0 }}>Prompt template</h3>
    <ul>
      <li>System message: define role, tone, and citation requirements.</li>
      <li>Human message: include the user question and formatted context snippets.</li>
      <li>Add explicit output rules (e.g., "If facts are missing, say you don't know.").</li>
    </ul>
    <p style={{ marginBottom: 0 }}>
      Follow the cookbook's recommendation to annotate each chunk with identifiers so you can cite sources back to the end user.
    </p>
  </div>
</section>

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 3rem', padding: '0 min(8vw, 96px)' }}>

```python
def answer_with_context(question: str, context_blocks: list[str]) -> str:
    prompt = "\n\n".join(
        [f"[{i}] {block}" for i, block in enumerate(context_blocks, start=1)]
    )
    response = client.chat.completions.create(
        model="claude-3-5-sonnet",
        messages=[
            {
                "role": "system",
                "content": "You are a concise support assistant. Cite sources like [1], [2].",
            },
            {
                "role": "user",
                "content": f"Question: {question}\n\nContext:\n{prompt}",
            },
        ],
        temperature=0.2,
        max_tokens=800,
    )
    return response.choices[0].message.content
```

<Note>
Use Haiku when you prioritize throughput; upgrade to Sonnet for harder questions or multilingual outputs.
</Note>

</section>

## Operational guidance

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 2rem', padding: '0 min(8vw, 96px)' }}>
  <div style={{
    padding: '2rem',
    borderRadius: '16px',
    background: 'linear-gradient(135deg, #d9e9ff 0%, #c5d9ff 25%, #aac3ff 50%, #7fa8ff 75%, #5b8fff 100%)',
    border: '1px solid rgba(255,255,255,0.12)',
    color: '#1a1a2e',
    marginBottom: '2rem'
  }}>
    <h3 style={{ marginTop: 0 }}>Best practices</h3>
    <ul>
      <li>Normalize text (lowercase, trim whitespace) before embedding.</li>
      <li>Refresh embeddings when source data changes; track versions in Postgres.</li>
      <li>Add automated evaluations (see our prompt patterns cookbook) to spot drift.</li>
      <li>Cache retrieval results for popular questions to reduce load.</li>
    </ul>
  </div>
</section>

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 3rem', padding: '0 min(8vw, 96px)' }}>

### Deployment on Heroku

- Run the embedding pipeline as a worker dyno; trigger rebuilds via Scheduler or webhooks.
- Store secrets (API keys, database URLs) in Config Vars, never in source.
- Use the `pgvector` extension on Heroku Postgres for similarity search.
- Stream chat responses to the client for faster perceived latency.

</section>
