---
title: "Pricing"
description: "Understand costs for Heroku Managed Inference and Agents models"
---

Heroku Managed Inference and Agents uses token-based pricing that aligns with the standard pricing from each model provider. You pay only for what you use, with no upfront costs or minimum commitments.

<Note>
  **Heroku Standard Pricing**: Heroku charges the same standard rates as the model providers (Anthropic, Cohere, Stability AI). There are no additional markups or hidden fees.
</Note>

## How Pricing Works

### Token-Based Billing

All models are priced based on tokens processed:

- **Input tokens**: Content you send to the model (prompts, context, images)
- **Output tokens**: Content the model generates (responses, completions)
- **Cached tokens**: When available, cached inputs cost less

### What is a Token?

Tokens are pieces of text that models process:

- **~4 characters** = 1 token (English)
- **~100 tokens** = 75 words
- **1,000 tokens** = ~750 words

**Example**:
```
"Hello, how are you today?" = ~6 tokens
```

### Billing Cycle

- Charges are calculated per API request
- Aggregated and billed monthly to your Heroku account
- View usage in real-time via Heroku Dashboard
- No minimum usage requirements

## Model Pricing Overview

<Tabs>
  <Tab title="Chat Models">
    Pricing is based on input and output tokens. Extended thinking (when enabled) incurs additional costs.

    ### Sonnet Models

    **Claude 4 Sonnet**
    - Input: Premium pricing per 1M tokens
    - Output: Premium pricing per 1M tokens
    - Extended Thinking: Additional cost per 1M tokens
    - Best for: Complex reasoning, coding, analysis

    **Claude 3.7 Sonnet**
    - Input: High pricing per 1M tokens
    - Output: High pricing per 1M tokens
    - Extended Thinking: Additional cost per 1M tokens
    - Best for: Research, advanced tasks

    **Claude 3.5 Sonnet**
    - Input: Moderate pricing per 1M tokens
    - Output: Moderate pricing per 1M tokens
    - Best for: General purpose, balanced quality

    ### Haiku Models

    **Claude 3.5 Haiku**
    - Input: Low pricing per 1M tokens
    - Output: Low pricing per 1M tokens
    - Best for: High volume, cost-sensitive applications

    **Claude 3.0 Haiku**
    - Input: Lowest pricing per 1M tokens
    - Output: Lowest pricing per 1M tokens
    - Best for: Maximum cost efficiency

    ### Amazon Nova Models

    **Amazon Nova Pro**
    - Input: Moderate pricing per 1M tokens
    - Output: Moderate pricing per 1M tokens
    - Best for: Enterprise applications

    **Amazon Nova Lite**
    - Input: Low pricing per 1M tokens
    - Output: Low pricing per 1M tokens
    - Best for: High-volume efficient processing

    ### OpenAI GPT OSS

    **OpenAI GPT OSS 120B**
    - Input: Moderate pricing per 1M tokens
    - Output: Moderate pricing per 1M tokens
    - Best for: Open-source projects

    <Note>
      **Exact pricing**: Contact Heroku sales or check your Heroku Dashboard for current per-token rates. Pricing follows the standard rates published by Anthropic, Amazon, and OpenAI.
    </Note>
  </Tab>

  <Tab title="Embedding Models">
    **Cohere Embed Multilingual**
    - Pricing per 1M tokens
    - Batch processing available (up to 96 strings)
    - Same cost regardless of language
    - Optimized for semantic search and RAG

    **Cost Optimization Tips**:
    - Batch multiple texts in single requests
    - Cache frequently used embeddings
    - Use appropriate input types for your use case
  </Tab>

  <Tab title="Image Models">
    **Stable Image Ultra**
    - Pricing per image generated
    - Cost varies by resolution
    - All aspect ratios same price for equivalent resolution
    - No charge for failed generations

    **Resolution Pricing**:
    - Standard resolutions (1024x1024): Base rate
    - Larger resolutions (1536x640): Higher rate
    - Smaller resolutions (768x1344): Lower rate

    **Cost per Image**: Fixed rate per successfully generated image
  </Tab>
</Tabs>

## Cost Comparison

### Chat Models Cost Efficiency

Relative cost per task (lower is better):

```
Claude 3.0 Haiku     ██░░░░░░░░░░░░░░░░░░ Most cost-efficient
Claude 3.5 Haiku     ███░░░░░░░░░░░░░░░░░ Very cost-efficient
Amazon Nova Lite     ████░░░░░░░░░░░░░░░░ Cost-efficient
Claude 3.5 Sonnet    ████████░░░░░░░░░░░░ Moderate
Amazon Nova Pro      ██████████░░░░░░░░░░ Moderate
Claude 3.7 Sonnet    ███████████████░░░░░ Higher cost
Claude 4 Sonnet      ████████████████████ Premium
```

### Example Costs

<AccordionGroup>
  <Accordion title="Customer Support Chatbot (10,000 conversations/month)">
    **Assumptions**:
    - Average 200 tokens input per conversation
    - Average 150 tokens output per conversation
    - 3.5M total tokens per month

    **Using Claude 3.5 Haiku** (recommended):
    - Optimized for high-volume, fast responses
    - Cost-effective for production scale
    - Estimated cost: Low monthly rate

    **Using Claude 3.5 Sonnet**:
    - Better reasoning, still reasonable cost
    - 3-4x cost of Haiku
    - Estimated cost: Moderate monthly rate
  </Accordion>

  <Accordion title="Document Analysis (1,000 documents/month)">
    **Assumptions**:
    - Average 50,000 tokens input per document
    - Average 2,000 tokens output (summary)
    - 52M total tokens per month

    **Using Claude 3.5 Sonnet** (recommended):
    - Vision capabilities for PDFs
    - Large context window
    - Estimated cost: Moderate monthly rate

    **Using Claude 4 Sonnet**:
    - Extended thinking for complex analysis
    - 2-3x cost of Sonnet 3.5
    - Estimated cost: Higher monthly rate
  </Accordion>

  <Accordion title="Semantic Search Application (100,000 queries/month)">
    **Assumptions**:
    - 100,000 query embeddings
    - 10,000 document embeddings (indexed once)
    - Average 100 tokens per embedding

    **Embedding costs** (Cohere Embed Multilingual):
    - One-time indexing of documents
    - Ongoing query embeddings
    - Estimated cost: Low monthly rate

    **Plus retrieval + generation** (Claude 3.5 Haiku):
    - 3 documents retrieved per query
    - 500 tokens input, 200 tokens output per query
    - Estimated cost: Moderate monthly rate

    **Total**: Combined moderate monthly cost
  </Accordion>

  <Accordion title="Image Generation (500 images/month)">
    **Using Stable Image Ultra**:
    - Fixed cost per successful image
    - Various resolutions supported
    - Estimated cost: Based on resolution and quantity

    **Cost Optimization**:
    - Use appropriate resolution for use case
    - Leverage negative prompts to reduce regenerations
    - Use seed for reproducibility
  </Accordion>
</AccordionGroup>

## Cost Optimization Strategies

### 1. Choose the Right Model

<AccordionGroup>
  <Accordion title="Use Haiku for High Volume">
    For customer support, simple Q&A, or high-frequency tasks:

    ```python
    # Cost-optimized approach
    if task_complexity == "simple":
        model = "claude-3-5-haiku"  # Most cost-efficient
    elif task_complexity == "moderate":
        model = "claude-3-5-sonnet"  # Balanced
    else:
        model = "claude-4-sonnet"  # For complex tasks only
    ```

    **Savings**: 5-10x compared to using premium models for all tasks
  </Accordion>

  <Accordion title="Route by Complexity">
    Automatically select models based on request characteristics:

    ```python
    def select_model(prompt_length, requires_vision, requires_thinking):
        if requires_thinking:
            return "claude-4-sonnet"
        elif requires_vision:
            return "claude-3-5-sonnet"
        elif prompt_length < 1000:
            return "claude-3-5-haiku"
        else:
            return "claude-3-5-sonnet"
    ```

    **Savings**: 3-5x by using appropriate models for each task
  </Accordion>
</AccordionGroup>

### 2. Optimize Token Usage

<AccordionGroup>
  <Accordion title="Set Max Token Limits">
    Control output length to avoid unnecessary tokens:

    ```python
    response = client.chat.completions.create(
        model="claude-3-5-sonnet",
        messages=messages,
        max_tokens=500  # Limit response length
    )
    ```

    **Savings**: 20-30% by preventing over-generation
  </Accordion>

  <Accordion title="Minimize Context">
    Only include necessary context in prompts:

    ```python
    # Bad: Sending entire document every time
    prompt = f"Document: {entire_10k_token_document}\nQuestion: {question}"

    # Good: Extract relevant sections first
    relevant_section = extract_relevant_content(document, question)
    prompt = f"Context: {relevant_section}\nQuestion: {question}"
    ```

    **Savings**: 50-70% by reducing input tokens
  </Accordion>

  <Accordion title="Use Caching">
    Cache responses for repeated queries:

    ```python
    import hashlib
    from functools import lru_cache

    @lru_cache(maxsize=1000)
    def get_cached_response(prompt_hash):
        # Only call API if not cached
        return call_api(prompt)

    # Use it
    prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
    response = get_cached_response(prompt_hash)
    ```

    **Savings**: 80-90% for repeated queries
  </Accordion>
</AccordionGroup>

### 3. Batch Operations

<AccordionGroup>
  <Accordion title="Batch Embeddings">
    Process multiple texts in a single request:

    ```python
    # Inefficient: Individual requests
    for text in texts:
        embedding = client.embeddings.create(
            model="cohere-embed-multilingual",
            input=[text]
        )

    # Efficient: Batch request
    embeddings = client.embeddings.create(
        model="cohere-embed-multilingual",
        input=texts  # Up to 96 texts
    )
    ```

    **Savings**: 50-60% fewer API calls, lower overhead
  </Accordion>

  <Accordion title="Async Processing">
    For non-real-time tasks, batch and process asynchronously:

    ```python
    import asyncio

    async def process_batch(texts):
        tasks = [generate_async(text) for text in texts]
        return await asyncio.gather(*tasks)

    # Process 100 texts efficiently
    results = asyncio.run(process_batch(texts))
    ```

    **Savings**: Better resource utilization, reduced costs
  </Accordion>
</AccordionGroup>

### 4. Monitor and Optimize

<AccordionGroup>
  <Accordion title="Track Usage by Feature">
    Monitor which features consume the most tokens:

    ```python
    import logging

    def track_usage(feature, input_tokens, output_tokens):
        cost = calculate_cost(input_tokens, output_tokens)
        logging.info(f"{feature}: {cost}")

    # Use it
    track_usage("chat", 500, 300)
    track_usage("analysis", 5000, 1000)
    ```

    **Benefit**: Identify expensive features for optimization
  </Accordion>

  <Accordion title="Set Budget Alerts">
    Configure alerts in Heroku Dashboard:
    - Monthly spend thresholds
    - Unusual usage patterns
    - Per-app limits

    **Benefit**: Avoid unexpected costs
  </Accordion>
</AccordionGroup>

## Viewing Your Usage

### Heroku Dashboard

1. Log into [Heroku Dashboard](https://dashboard.heroku.com)
2. Navigate to your app
3. View "Resources" tab
4. Click on "Heroku Managed Inference and Agents"
5. See real-time usage and costs

### Via CLI

```bash
# View current usage
heroku ai:models:info --app my-app

# Check all models attached
heroku addons:info heroku-inference --app my-app
```

### Usage Metrics Available

- **Total tokens processed**: Input + output + extended thinking
- **Requests per day/month**: API call volume
- **Cost breakdown**: By model and endpoint
- **Rate limit status**: Current usage vs. limits

## Billing and Invoicing

### How You're Billed

- **Monthly billing**: Charged to your Heroku account
- **Per-app tracking**: Usage tracked per application
- **Consolidated invoice**: All Heroku services in one bill
- **Detailed breakdown**: Usage by model and resource

### Payment Methods

- Credit card
- PayPal
- Invoice billing (Enterprise customers)

### Tax Information

- Charges may be subject to applicable taxes
- Tax rates vary by location
- View tax details in your invoice

## Rate Limits and Quotas

Rate limits prevent abuse and ensure fair usage:

### Default Limits

- **Requests per minute**: Varies by model
- **Tokens per minute**: Based on plan
- **Concurrent requests**: Maximum simultaneous calls

### Increasing Limits

For higher limits:
1. Upgrade your Heroku plan
2. Contact Heroku support
3. Demonstrate production usage patterns

<Note>
  **Rate limits**: Exact limits depend on your Heroku plan and model. Check your dashboard for current limits.
</Note>

## Enterprise Pricing

For large-scale deployments:

- Custom pricing available
- Volume discounts
- Dedicated support
- SLA guarantees
- Private model deployments

**Contact**: Reach out to Heroku sales for enterprise pricing

## Frequently Asked Questions

<AccordionGroup>
  <Accordion title="Are there any hidden fees?">
    No. Heroku charges the standard model provider rates with no markups or hidden fees. You pay only for:
    - Tokens processed (input + output)
    - Images generated
    - Standard Heroku platform costs (dynos, add-ons)
  </Accordion>

  <Accordion title="Do I pay for failed requests?">
    No. You're only charged for successful API calls that process tokens. Failed requests due to:
    - Invalid parameters
    - Authentication errors
    - Rate limit errors

    These do not incur charges.
  </Accordion>

  <Accordion title="How do I estimate my monthly costs?">
    1. Estimate request volume (requests per month)
    2. Estimate average tokens per request
    3. Choose your model
    4. Calculate: requests × tokens × rate per token

    Example:
    - 10,000 requests/month
    - 500 input + 300 output tokens average
    - Using Claude 3.5 Haiku
    - Total: Calculable monthly cost
  </Accordion>

  <Accordion title="Can I set a spending limit?">
    Yes. Configure budget alerts in your Heroku Dashboard:
    - Monthly spend threshold
    - Email notifications
    - Optional hard limits (Enterprise)
  </Accordion>

  <Accordion title="What happens if I exceed my rate limit?">
    Requests that exceed rate limits:
    - Return a 429 error
    - Not charged
    - Can be retried after the limit resets

    Implement exponential backoff for handling rate limits.
  </Accordion>

  <Accordion title="Is caching charged?">
    Cache usage varies:
    - Client-side caching: No additional charge
    - Model-level caching (when available): Reduced token costs
    - Your application's caching: Standard infrastructure costs
  </Accordion>
</AccordionGroup>

## Related Resources

<CardGroup cols={2}>
  <Card title="Models overview" href="/inference-api/models" icon="sparkles">
    Compare all available models
  </Card>
  <Card title="Choosing a model" href="/inference-api/choosing-a-model" icon="lightbulb">
    Select cost-effective models
  </Card>
  <Card title="AI Studio" href="/heroku-inference/ai-studio" icon="palette">
    Test models before committing
  </Card>
  <Card title="CLI Commands" href="/inference-api/cli-commands" icon="terminal">
    Manage model resources
  </Card>
</CardGroup>
