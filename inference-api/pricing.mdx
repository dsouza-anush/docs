---
title: "Pricing"
description: "Understand costs for Heroku Managed Inference and Agents models"
---

Heroku Managed Inference and Agents uses token-based pricing that matches the standard rates from each model provider. You pay only for what you use, with no upfront costs or minimum commitments.

<Note>
  **Heroku Standard Pricing**: Heroku charges the same standard rates as the model providers (Anthropic, Cohere, Stability AI). There are no additional markups or hidden fees.
</Note>

## How pricing works

### Token-based billing

All models are priced based on tokens processed:

- **Input tokens**: Content you send to the model (prompts, context, images)
- **Output tokens**: Content the model generates (responses, completions)
- **Cached tokens**: When available, cached inputs cost less

### What counts as a token?

Tokens are pieces of text that models process:

- **~4 characters** = 1 token (English)
- **~100 tokens** = 75 words
- **1,000 tokens** = ~750 words

**Example**
```
"Hello, how are you today?" ≈ 6 tokens
```

### Billing cycle

- Charges are calculated per API request.
- Usage is aggregated and billed monthly to your Heroku account.
- View usage in real time in the Heroku Dashboard.
- There are no minimum usage requirements.

## Model Pricing Overview

<Tabs>
  <Tab title="Chat Models">
    Pricing is based on input and output tokens. Extended thinking (when enabled) incurs additional costs.

    | Model | Input pricing | Output pricing | Extended thinking | Provider reference |
    | --- | --- | --- | --- | --- |
    | Claude 4 Sonnet | Matches Anthropic published rate | Matches Anthropic published rate | Supported | [Anthropic pricing](https://www.anthropic.com/pricing) |
    | Claude 3.7 Sonnet | Matches Anthropic published rate | Matches Anthropic published rate | Supported | [Anthropic pricing](https://www.anthropic.com/pricing) |
    | Claude 3.5 Sonnet | Matches Anthropic published rate | Matches Anthropic published rate | Supported | [Anthropic pricing](https://www.anthropic.com/pricing) |
    | Claude 3.5 Haiku | Matches Anthropic published rate | Matches Anthropic published rate | Not available | [Anthropic pricing](https://www.anthropic.com/pricing) |
    | Claude 3.0 Haiku | Matches Anthropic published rate | Matches Anthropic published rate | Not available | [Anthropic pricing](https://www.anthropic.com/pricing) |
    | Amazon Nova Pro | Matches AWS Bedrock rate | Matches AWS Bedrock rate | Not available | [AWS Bedrock pricing](https://aws.amazon.com/bedrock/pricing/) |
    | Amazon Nova Lite | Matches AWS Bedrock rate | Matches AWS Bedrock rate | Not available | [AWS Bedrock pricing](https://aws.amazon.com/bedrock/pricing/) |
    | OpenAI GPT OSS 120B | Matches published open-source rate | Matches published open-source rate | Not available | [Monitor in Heroku Dashboard](https://dashboard.heroku.com/) |

    **Best-fit guidance**
    - Claude Sonnet models: balanced or advanced reasoning with extended thinking support.
    - Claude Haiku models: latency-sensitive, cost-optimized workloads.
    - Amazon Nova: workloads standardized on AWS Bedrock billing.
    - GPT OSS 120B: experimentation and custom deployments with open-source weights.

    <Tip>
      Extended thinking increases output token usage. Enable it only for prompts that benefit from deeper reasoning.
    </Tip>
  </Tab>

  <Tab title="Embedding Models">
    | Model | Pricing unit | Notes | Provider reference |
    | --- | --- | --- | --- |
    | Cohere Embed Multilingual | Matches Cohere rate per 1M tokens | Batch up to 96 strings per request | [Cohere pricing](https://cohere.com/pricing) |

    **Cost optimization**
    - Batch multiple strings to reduce request overhead.
    - Cache frequently accessed embeddings.
    - Use the appropriate input type (`search_document`, `search_query`, `classification`, `clustering`) for the task.
  </Tab>

  <Tab title="Image Models">
    | Model | Pricing unit | Resolution impact | Provider reference |
    | --- | --- | --- | --- |
    | Stable Image Ultra | Matches Stability rate per generated image | Higher resolutions cost more; aspect ratio does not change price at a given resolution | [Stability pricing](https://platform.stability.ai/pricing) |

    **Cost optimization**
    - Test prompts with draft-quality renders before generating production assets.
    - Reuse negative prompts to avoid generating unusable images.
    - Standardize on a small set of resolutions for easier budgeting.
  </Tab>
</Tabs>

## Cost Comparison

### Chat Models Cost Efficiency

Relative cost per task (lower is better):

```
Claude 3.0 Haiku     ██░░░░░░░░░░░░░░░░░░ Most cost-efficient
Claude 3.5 Haiku     ███░░░░░░░░░░░░░░░░░ Very cost-efficient
Amazon Nova Lite     ████░░░░░░░░░░░░░░░░ Cost-efficient
Claude 3.5 Sonnet    ████████░░░░░░░░░░░░ Moderate
Amazon Nova Pro      ██████████░░░░░░░░░░ Moderate
Claude 3.7 Sonnet    ███████████████░░░░░ Higher cost
Claude 4 Sonnet      ████████████████████ Premium
```

### Example costs

<AccordionGroup>
  <Accordion title="Customer support chatbot (10,000 conversations/month)">
    **Assumptions**
    - 200 input tokens per conversation
    - 150 output tokens per conversation
    - 3.5M total tokens per month (2.0M input + 1.5M output)

    **Claude 3.5 Haiku (recommended)**
    - Lowest latency and cost within the Claude lineup.
    - Monthly cost ≈ provider token rates × (2.0M input + 1.5M output).

    **Claude 3.5 Sonnet (upgrade path)**
    - Improves reasoning for complex conversations.
    - Expect roughly 3–4× the Haiku cost for the same workload.
  </Accordion>

  <Accordion title="Document analysis (1,000 documents/month)">
    **Assumptions**
    - 50,000 input tokens per document
    - 2,000 output tokens (summary)
    - 52M total tokens per month (50M input + 2M output)

    **Claude 3.5 Sonnet (recommended)**
    - Handles long documents with a 200K context window and vision support.
    - Apply provider token rates to 50M input + 2M output.

    **Claude 4 Sonnet (extended thinking)**
    - Use when documents require deeper analysis.
    - Add extended thinking output tokens to your cost forecast.
  </Accordion>

  <Accordion title="Semantic search application (100,000 queries/month)">
    **Assumptions**
    - 100,000 query embeddings
    - 10,000 document embeddings (indexed once)
    - 100 tokens per embedding

    **Embedding costs (Cohere Embed Multilingual)**
    - One-time document indexing + ongoing query embeddings.
    - Monthly cost ≈ provider rate × total embedding tokens.

    **Retrieval + generation (Claude 3.5 Haiku)**
    - 3 documents retrieved per query (RAG).
    - 500 input + 200 output tokens per answer.
    - Multiply provider token rates by monthly token totals to project spend.
  </Accordion>

  <Accordion title="Image generation (500 images/month)">
    **Assumptions**
    - 500 successful generations at 1024×1024 resolution
    - Occasional reruns for prompt refinement

    **Stable Image Ultra**
    - Cost per image matches Stability’s published rate.
    - Increase resolution (for example 1536×640) when you need higher fidelity; expect a proportional cost increase.

    **Optimization tips**
    - Validate prompts with draft-quality renders.
    - Reuse seeds and negative prompts to reduce retries.
  </Accordion>
</AccordionGroup>

## Build a forecast

1. **Retrieve current rates** from the provider links in the tables above.
2. **Estimate monthly tokens**: add input + output + cached tokens (if applicable).
3. **Apply the rates** to each token bucket and sum the results.
4. **Include feature surcharges** such as extended thinking or higher image resolutions.
5. **Validate in production** using the Heroku Dashboard (`Resources` → `Heroku Inference` → `View Metrics`).

<Tip>
  Keep prompts concise and set `max_tokens` to control output length—small tweaks can meaningfully reduce spend at scale.
</Tip>

## Cost Optimization Strategies

### 1. Choose the Right Model

<AccordionGroup>
  <Accordion title="Use Haiku for High Volume">
    For customer support, simple Q&A, or high-frequency tasks:

    ```python
    # Cost-optimized approach
    if task_complexity == "simple":
        model = "claude-3-5-haiku"  # Most cost-efficient
    elif task_complexity == "moderate":
        model = "claude-3-5-sonnet"  # Balanced
    else:
        model = "claude-4-sonnet"  # For complex tasks only
    ```

    **Savings**: 5-10x compared to using premium models for all tasks
  </Accordion>

  <Accordion title="Route by Complexity">
    Automatically select models based on request characteristics:

    ```python
    def select_model(prompt_length, requires_vision, requires_thinking):
        if requires_thinking:
            return "claude-4-sonnet"
        elif requires_vision:
            return "claude-3-5-sonnet"
        elif prompt_length < 1000:
            return "claude-3-5-haiku"
        else:
            return "claude-3-5-sonnet"
    ```

    **Savings**: 3-5x by using appropriate models for each task
  </Accordion>
</AccordionGroup>

### 2. Optimize Token Usage

<AccordionGroup>
  <Accordion title="Set Max Token Limits">
    Control output length to avoid unnecessary tokens:

    ```python
    response = client.chat.completions.create(
        model="claude-3-5-sonnet",
        messages=messages,
        max_tokens=500  # Limit response length
    )
    ```

    **Savings**: 20-30% by preventing over-generation
  </Accordion>

  <Accordion title="Minimize Context">
    Only include necessary context in prompts:

    ```python
    # Bad: Sending entire document every time
    prompt = f"Document: {entire_10k_token_document}\nQuestion: {question}"

    # Good: Extract relevant sections first
    relevant_section = extract_relevant_content(document, question)
    prompt = f"Context: {relevant_section}\nQuestion: {question}"
    ```

    **Savings**: 50-70% by reducing input tokens
  </Accordion>

  <Accordion title="Use Caching">
    Cache responses for repeated queries:

    ```python
    import hashlib
    from functools import lru_cache

    @lru_cache(maxsize=1000)
    def get_cached_response(prompt_hash):
        # Only call API if not cached
        return call_api(prompt)

    # Use it
    prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
    response = get_cached_response(prompt_hash)
    ```

    **Savings**: 80-90% for repeated queries
  </Accordion>
</AccordionGroup>

### 3. Batch Operations

<AccordionGroup>
  <Accordion title="Batch Embeddings">
    Process multiple texts in a single request:

    ```python
    # Inefficient: Individual requests
    for text in texts:
        embedding = client.embeddings.create(
            model="cohere-embed-multilingual",
            input=[text]
        )

    # Efficient: Batch request
    embeddings = client.embeddings.create(
        model="cohere-embed-multilingual",
        input=texts  # Up to 96 texts
    )
    ```

    **Savings**: 50-60% fewer API calls, lower overhead
  </Accordion>

  <Accordion title="Async Processing">
    For non-real-time tasks, batch and process asynchronously:

    ```python
    import asyncio

    async def process_batch(texts):
        tasks = [generate_async(text) for text in texts]
        return await asyncio.gather(*tasks)

    # Process 100 texts efficiently
    results = asyncio.run(process_batch(texts))
    ```

    **Savings**: Better resource utilization, reduced costs
  </Accordion>
</AccordionGroup>

### 4. Monitor and Optimize

<AccordionGroup>
  <Accordion title="Track Usage by Feature">
    Monitor which features consume the most tokens:

    ```python
    import logging

    def track_usage(feature, input_tokens, output_tokens):
        cost = calculate_cost(input_tokens, output_tokens)
        logging.info(f"{feature}: {cost}")

    # Use it
    track_usage("chat", 500, 300)
    track_usage("analysis", 5000, 1000)
    ```

    **Benefit**: Identify expensive features for optimization
  </Accordion>

  <Accordion title="Set Budget Alerts">
    Configure alerts in Heroku Dashboard:
    - Monthly spend thresholds
    - Unusual usage patterns
    - Per-app limits

    **Benefit**: Avoid unexpected costs
  </Accordion>
</AccordionGroup>

## Viewing Your Usage

### Heroku Dashboard

1. Log into [Heroku Dashboard](https://dashboard.heroku.com)
2. Navigate to your app
3. View "Resources" tab
4. Click on "Heroku Managed Inference and Agents"
5. See real-time usage and costs

### Via CLI

```bash
# View current usage
heroku ai:models:info --app my-app

# Check all models attached
heroku addons:info heroku-inference --app my-app
```

### Usage Metrics Available

- **Total tokens processed**: Input + output + extended thinking
- **Requests per day/month**: API call volume
- **Cost breakdown**: By model and endpoint
- **Rate limit status**: Current usage vs. limits

## Billing and Invoicing

### How You're Billed

- **Monthly billing**: Charged to your Heroku account
- **Per-app tracking**: Usage tracked per application
- **Consolidated invoice**: All Heroku services in one bill
- **Detailed breakdown**: Usage by model and resource

### Payment Methods

- Credit card
- PayPal
- Invoice billing (Enterprise customers)

### Tax Information

- Charges may be subject to applicable taxes
- Tax rates vary by location
- View tax details in your invoice

## Rate Limits and Quotas

Rate limits prevent abuse and ensure fair usage:

### Default Limits

- **Requests per minute**: Varies by model
- **Tokens per minute**: Based on plan
- **Concurrent requests**: Maximum simultaneous calls

### Increasing Limits

For higher limits:
1. Upgrade your Heroku plan
2. Contact Heroku support
3. Demonstrate production usage patterns

<Note>
  **Rate limits**: Exact limits depend on your Heroku plan and model. Check your dashboard for current limits.
</Note>

## Enterprise Pricing

For large-scale deployments:

- Custom pricing available
- Volume discounts
- Dedicated support
- SLA guarantees
- Private model deployments

**Contact**: Reach out to Heroku sales for enterprise pricing

## Frequently Asked Questions

<AccordionGroup>
  <Accordion title="Are there any hidden fees?">
    No. Heroku charges the standard model provider rates with no markups or hidden fees. You pay only for:
    - Tokens processed (input + output)
    - Images generated
    - Standard Heroku platform costs (dynos, add-ons)
  </Accordion>

  <Accordion title="Do I pay for failed requests?">
    No. You're only charged for successful API calls that process tokens. Failed requests due to:
    - Invalid parameters
    - Authentication errors
    - Rate limit errors

    These do not incur charges.
  </Accordion>

  <Accordion title="How do I estimate my monthly costs?">
    1. Estimate request volume (requests per month)
    2. Estimate average tokens per request
    3. Choose your model
    4. Calculate: requests × tokens × rate per token

    Example:
    - 10,000 requests/month
    - 500 input + 300 output tokens average
    - Using Claude 3.5 Haiku
    - Total: Calculable monthly cost
  </Accordion>

  <Accordion title="Can I set a spending limit?">
    Yes. Configure budget alerts in your Heroku Dashboard:
    - Monthly spend threshold
    - Email notifications
    - Optional hard limits (Enterprise)
  </Accordion>

  <Accordion title="What happens if I exceed my rate limit?">
    Requests that exceed rate limits:
    - Return a 429 error
    - Not charged
    - Can be retried after the limit resets

    Implement exponential backoff for handling rate limits.
  </Accordion>

  <Accordion title="Is caching charged?">
    Cache usage varies:
    - Client-side caching: No additional charge
    - Model-level caching (when available): Reduced token costs
    - Your application's caching: Standard infrastructure costs
  </Accordion>
</AccordionGroup>

## Related Resources

<CardGroup cols={2}>
  <Card title="Models overview" href="/inference-api/models" icon="sparkles">
    Compare all available models
  </Card>
  <Card title="Choosing a model" href="/inference-api/choosing-a-model" icon="lightbulb">
    Select cost-effective models
  </Card>
  <Card title="AI Studio" href="/heroku-inference/ai-studio" icon="palette">
    Test models before committing
  </Card>
  <Card title="CLI Commands" href="/inference-api/cli-commands" icon="terminal">
    Manage model resources
  </Card>
</CardGroup>
