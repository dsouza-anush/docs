---
title: "Models overview"
description: "Choose the right Heroku AI model using side-by-side comparisons, playbooks, and provisioning tips"
fullWidth: true
---

<section
  style={{
    margin: '0 calc(-1 * min(8vw, 96px)) 3rem',
    padding: '4rem min(8vw, 96px)',
    background: 'linear-gradient(180deg, rgba(35,30,60,0.65) 0%, rgba(10,8,20,0) 100%)',
    borderRadius: '2.25rem',
    border: '1px solid rgba(255,255,255,0.05)',
    position: 'relative',
    overflow: 'hidden'
  }}
>
  <div style={{ position: 'absolute', inset: '0', background: 'radial-gradient(circle at 15% 25%, rgba(139,127,212,0.35), transparent 60%)' }} />
  <div style={{ position: 'absolute', inset: '0', background: 'radial-gradient(circle at 75% 20%, rgba(86,188,255,0.25), transparent 65%)' }} />
  <div style={{ position: 'relative', display: 'grid', gridTemplateColumns: 'repeat(auto-fit, minmax(320px, 1fr))', gap: '3rem', alignItems: 'center' }}>
    <div>
      <p style={{ letterSpacing: '0.12em', textTransform: 'uppercase', fontSize: '0.82rem', color: 'rgba(235,235,255,0.6)' }}>Catalog overview</p>
      <h1 style={{ fontSize: '2.6rem', lineHeight: 1.1, margin: '0.5rem 0 1rem' }}>Pick the best model for every experience</h1>
      <p style={{ fontSize: '1.05rem', lineHeight: 1.6, color: 'rgba(235,235,255,0.8)', maxWidth: '34rem' }}>
        Heroku Managed Inference and Agents exposes leading chat, embedding, and generative image models behind OpenAI-compatible endpoints.
        Use these cards and comparison tools to balance quality, performance, and spend.
      </p>
    </div>
    <div style={{
      padding: '2rem',
      borderRadius: '1.75rem',
      background: 'rgba(12,10,24,0.55)',
      border: '1px solid rgba(255,255,255,0.06)',
      boxShadow: '0 26px 60px rgba(20,18,40,0.4)',
      backdropFilter: 'blur(18px)'
    }}>
      <p style={{ fontSize: '0.85rem', color: 'rgba(235,235,255,0.65)', marginBottom: '1rem' }}>Model selection checklist</p>
      <ul style={{ listStyle: 'disc', paddingLeft: '1.25rem', lineHeight: 1.6, color: 'rgba(235,235,255,0.82)' }}>
        <li>Prototype with Claude 4.5 Haiku or Nova Lite for fast, cost-effective validation.</li>
        <li>Upgrade to Claude 4.5 Sonnet for balanced performance on complex tasks.</li>
        <li>Reach for Claude 4 Sonnet when workflows demand deep reasoning.</li>
        <li>Use Cohere Embed Multilingual to add semantic search and retrieval.</li>
      </ul>
    </div>
  </div>
</section>

## Featured models

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 3rem', padding: '0 min(8vw, 96px)' }}>
  <div
    style={{
      display: 'grid',
      gap: '1.75rem',
      gridTemplateColumns: 'repeat(auto-fit, minmax(260px, 1fr))'
    }}
  >
    <a
      href="#chat-models"
      style={{
        textDecoration: 'none',
        padding: '2.3rem 2.1rem',
        borderRadius: '1.75rem',
        color: '#F9FAFF',
        background: 'linear-gradient(160deg, rgba(105,90,255,0.58) 0%, rgba(72,60,200,0.52) 40%, rgba(35,30,110,0.72) 100%)',
        boxShadow: '0 26px 60px rgba(80,68,220,0.23)',
        border: '1px solid rgba(255,255,255,0.1)',
        backdropFilter: 'blur(18px)',
        display: 'flex',
        flexDirection: 'column',
        gap: '0.9rem'
      }}
    >
      <div style={{ fontSize: '0.82rem', letterSpacing: '0.08em', textTransform: 'uppercase', opacity: 0.85 }}>Claude 4.5 Sonnet</div>
      <div style={{ fontSize: '1.12rem', fontWeight: 600, lineHeight: 1.32 }}>
        High-performance model balancing intelligence and speed for complex tasks.
      </div>
      <div style={{ fontSize: '0.92rem', opacity: 0.85 }}>Perfect for data processing, sales forecasting, and nuanced content generation.</div>
      <span style={{ fontSize: '0.88rem', opacity: 0.8 }}>View details →</span>
    </a>

    <a
      href="#chat-models"
      style={{
        textDecoration: 'none',
        padding: '2.3rem 2.1rem',
        borderRadius: '1.75rem',
        color: '#F9FAFF',
        background: 'linear-gradient(160deg, rgba(41,163,206,0.54) 0%, rgba(47,118,192,0.5) 40%, rgba(22,54,120,0.66) 100%)',
        boxShadow: '0 26px 60px rgba(36,140,200,0.22)',
        border: '1px solid rgba(255,255,255,0.1)',
        backdropFilter: 'blur(18px)',
        display: 'flex',
        flexDirection: 'column',
        gap: '0.9rem'
      }}
    >
      <div style={{ fontSize: '0.82rem', letterSpacing: '0.08em', textTransform: 'uppercase', opacity: 0.85 }}>Claude 4.5 Haiku</div>
      <div style={{ fontSize: '1.12rem', fontWeight: 600, lineHeight: 1.32 }}>
        Fast and highly cost-effective model optimized for high-throughput tasks.
      </div>
      <div style={{ fontSize: '0.92rem', opacity: 0.85 }}>Ideal for rapid responses, content moderation, and inventory management.</div>
      <span style={{ fontSize: '0.88rem', opacity: 0.8 }}>View details →</span>
    </a>

    <a
      href="#image-models"
      style={{
        textDecoration: 'none',
        padding: '2.3rem 2.1rem',
        borderRadius: '1.75rem',
        color: '#F9FAFF',
        background: 'linear-gradient(160deg, rgba(255,148,92,0.58) 0%, rgba(223,104,60,0.52) 45%, rgba(150,54,28,0.7) 100%)',
        boxShadow: '0 26px 60px rgba(255,136,76,0.22)',
        border: '1px solid rgba(255,255,255,0.1)',
        backdropFilter: 'blur(18px)',
        display: 'flex',
        flexDirection: 'column',
        gap: '0.9rem'
      }}
    >
      <div style={{ fontSize: '0.82rem', letterSpacing: '0.08em', textTransform: 'uppercase', opacity: 0.85 }}>Stable Image Ultra</div>
      <div style={{ fontSize: '1.12rem', fontWeight: 600, lineHeight: 1.32 }}>
        High-fidelity diffusion with prompt adherence, aspect ratio control, and seed replay.
      </div>
      <div style={{ fontSize: '0.92rem', opacity: 0.85 }}>Perfect for brand assets, product renders, and campaign variants.</div>
      <span style={{ fontSize: '0.88rem', opacity: 0.8 }}>View details →</span>
    </a>
  </div>
</section>

## Compare models at a glance

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 2.5rem', padding: '0 min(8vw, 96px)' }}>
  | Model | Category | Strength | Relative cost | Context window | Extended thinking | Vision | Link |
  | --- | --- | --- | --- | --- | --- | --- | --- |
  | Claude 4.5 Sonnet | Chat | High-performance balanced model with speed | Balanced | 200K tokens | ✓ | ✓ | [Pricing](/inference-api/pricing#model-pricing-overview) |
  | Claude 4 Sonnet | Chat | Deep reasoning with chain-of-thought | Premium | 200K tokens | ✓ | ✓ | [Pricing](/inference-api/pricing#model-pricing-overview) |
  | Claude 3.7 Sonnet | Chat | High intelligence with extended thinking | High | 200K tokens | ✓ | ✓ | [Pricing](/inference-api/pricing#model-pricing-overview) |
  | Claude 3.5 Sonnet | Chat | Balanced quality, speed, and cost | Balanced | 200K tokens | Optional | ✓ | [Pricing](/inference-api/pricing#model-pricing-overview) |
  | Claude 4.5 Haiku | Chat | Fast and highly cost-effective | Low | 200K tokens | ✗ | ✗ | [Pricing](/inference-api/pricing#model-pricing-overview) |
  | Claude 3.5 Haiku | Chat | Lowest latency, great for scale | Low | 200K tokens | ✗ | ✗ | [Pricing](/inference-api/pricing#model-pricing-overview) |
  | Claude 3.0 Haiku | Chat | Legacy low-cost option | Low | 200K tokens | ✗ | ✗ | [Pricing](/inference-api/pricing#model-pricing-overview) |
  | Amazon Nova Pro | Chat | Enterprise-first generalist | High | Large | ✗ | ✗ | [Pricing](/inference-api/pricing#model-pricing-overview) |
  | Amazon Nova Lite | Chat | Efficient conversational model | Low | Medium | ✗ | ✗ | [Pricing](/inference-api/pricing#model-pricing-overview) |
  | OpenAI GPT OSS 120B | Chat | Open-weight experimentation | Self-managed | 128K tokens | ✗ | ✗ | [Pricing](/inference-api/pricing#model-pricing-overview) |
  | Cohere Embed Multilingual | Embeddings | Multilingual semantic search | Per 1M tokens | — | — | — | [Pricing](/inference-api/pricing#model-pricing-overview) |
  | Stable Image Ultra | Image | High fidelity image synthesis | Per image | — | — | — | [Pricing](/inference-api/pricing#model-pricing-overview) |
</section>

<Note>
  Cost guidance reflects relative position within the Heroku AI catalog. Check the pricing page for live token and image rates before deploying to production.
</Note>

## Chat models

Advanced language models for assistants, copilots, and automated workflows.

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 3rem', padding: '0 min(8vw, 96px)' }}>
<AccordionGroup>
  <Accordion title="Claude 4.5 Sonnet (high-performance balance)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Latest Sonnet model balancing intelligence and speed.</li>
          <li>Multimodal: accepts images, PDFs, and mixed media.</li>
          <li>200K token window for large documents and code bases.</li>
          <li>Supports extended thinking, tool calling, and structured outputs.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Complex tasks requiring both speed and quality.</li>
          <li>Data processing and sales forecasting workflows.</li>
          <li>Nuanced content generation for enterprise applications.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Optimized for high-throughput tasks and real-time interactions.</li>
          <li>Use for applications requiring rapid responses and content moderation.</li>
          <li>Stream responses to maintain low latency for end users.</li>
        </ul>
        <Callout title="Model ID">
          `claude-4-5-sonnet`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="Claude 4 Sonnet (premium reasoning)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Extended thinking unlocks deeper multi-step reasoning.</li>
          <li>Multimodal: accepts images, PDFs, and mixed media.</li>
          <li>200K token window for large documents and code bases.</li>
          <li>Supports tool calling and structured outputs.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Strategic analysis and research copilots.</li>
          <li>Complex coding assistants and debugging flows.</li>
          <li>Document intelligence use cases requiring full fidelity.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Enable extended thinking only when needed to manage output spend.</li>
          <li>Use evaluation harnesses in AI Studio before promoting new prompts.</li>
          <li>Stream responses to keep latency manageable for end users.</li>
        </ul>
        <Callout title="Model ID">
          `claude-4-sonnet`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="Claude 3.7 Sonnet (high IQ, lower cost)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Extended thinking for difficult reasoning with reduced pricing.</li>
          <li>Vision support and 200K context window.</li>
          <li>Fast enough for most production assistants.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Technical documentation Q&A bots.</li>
          <li>Code review companions.</li>
          <li>Enterprise knowledge management workflows.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Instrument latency metrics; responses are slightly faster than Claude 4.</li>
          <li>Reserve extended thinking for escalations to balance cost.</li>
          <li>Attach the model to staging apps to validate prompts before rollout.</li>
        </ul>
        <Callout title="Model ID">
          `claude-3-7-sonnet`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="Claude 3.5 Sonnet (balanced default)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Strong reasoning with faster average latency than Claude 4.</li>
          <li>Optional extended thinking when prompts demand it.</li>
          <li>Vision for OCR, screenshot analysis, and product imagery.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>General-purpose product assistants.</li>
          <li>Code generation where turnaround speed matters.</li>
          <li>Marketing content with light creative editing.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Set `max_tokens` between 600–1200 to keep responses snappy.</li>
          <li>Toggle streaming for user-facing chat experiences.</li>
          <li>Use as the baseline model in evaluation suites.</li>
        </ul>
        <Callout title="Model ID">
          `claude-3-5-sonnet`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="Claude 4.5 Haiku (fast and cost-effective)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Fast and highly cost-effective for high-throughput tasks.</li>
          <li>Optimized for rapid responses and real-time interactions.</li>
          <li>Full 200K context window for broad prompts.</li>
          <li>Perfect for content moderation and inventory management.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Applications requiring rapid responses.</li>
          <li>Content moderation at scale.</li>
          <li>Inventory management and high-volume automation.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Optimized for high-throughput with low latency.</li>
          <li>No extended thinking—design prompts accordingly.</li>
          <li>Batch inference when possible to maximize concurrency.</li>
        </ul>
        <Callout title="Model ID">
          `claude-4-5-haiku`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="Claude 3.5 Haiku (high-volume choice)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Lowest latency across the Claude family.</li>
          <li>Full 200K context window for broad prompts.</li>
          <li>Optimized for throughput and predictable billing.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Customer support automations.</li>
          <li>Moderation or classification pipelines.</li>
          <li>Broadcast messaging, notifications, and templated replies.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>No extended thinking—design prompts accordingly.</li>
          <li>Batch inference when possible to maximize concurrency.</li>
          <li>Monitor token mix; output remains capped at 4K tokens.</li>
        </ul>
        <Callout title="Model ID">
          `claude-3-5-haiku`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="Claude 3.0 Haiku (legacy, low cost)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Ultra-low latency for simple prompts.</li>
          <li>Minimal cost overhead for pilot projects.</li>
          <li>Compatible with existing Haiku workloads.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Lightweight Q&A and FAQ bots.</li>
          <li>Content moderation stubs.</li>
          <li>Legacy apps migrating from older Anthropic versions.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Plan migration to 3.5 Haiku for better quality.</li>
          <li>Limit prompts to short instructions to maintain output quality.</li>
          <li>Keep an eye on model deprecation timelines.</li>
        </ul>
        <Callout title="Model ID">
          `claude-3-haiku`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="Amazon Nova Pro (enterprise generalist)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Strong reasoning with AWS-native integrations.</li>
          <li>Extended context for multilingual and domain-heavy prompts.</li>
          <li>Governance aligned with AWS Bedrock controls.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Workloads already standardized on AWS security tooling.</li>
          <li>Enterprise knowledge bases requiring PII guardrails.</li>
          <li>Finance and regulated industry copilots.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Cross-check request volumes with AWS cost allocation tags.</li>
          <li>Rely on Bedrock safety settings exposed via model parameters.</li>
          <li>Coordinate upgrades with AWS release cadence.</li>
        </ul>
        <Callout title="Model ID">
          `amazon-nova-pro`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="Amazon Nova Lite (efficient chat)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Lower price point compared to Nova Pro.</li>
          <li>Optimized for general conversational tasks.</li>
          <li>Shorter latency with manageable quality trade-offs.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Customer service and triage bots.</li>
          <li>Internal productivity copilots.</li>
          <li>High-volume prompts within the AWS ecosystem.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Use shorter prompts to keep responses sharp.</li>
          <li>Pair with Claude models for fallback escalation flows.</li>
          <li>Benchmark throughput against Haiku to choose the lowest-cost option.</li>
        </ul>
        <Callout title="Model ID">
          `amazon-nova-lite`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="OpenAI GPT OSS 120B (open-weight sandbox)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Open-source friendly architecture and weights.</li>
          <li>Supports custom fine-tuning and adapters.</li>
          <li>128K context window for broader experimentation.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Research teams evaluating open models.</li>
          <li>Hybrid setups mixing managed and self-hosted inference.</li>
          <li>Education and experimentation environments.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Expect higher latency compared to hosted Claude models.</li>
          <li>Budget for additional evaluation since quality varies by prompt.</li>
          <li>Track GPU usage via Heroku metrics to control costs.</li>
        </ul>
        <Callout title="Model ID">
          `openai-gpt-oss-120b`
        </Callout>
      </div>
    </Columns>
  </Accordion>
</AccordionGroup>
</section>

## Embedding models

Use embeddings for semantic search, classification, clustering, and retrieval-augmented generation.

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 3rem', padding: '0 min(8vw, 96px)' }}>
<AccordionGroup>
  <Accordion title="Cohere Embed Multilingual">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>100+ language support with 1,024-dimension vectors.</li>
          <li>Optimized presets for search, classification, and clustering.</li>
          <li>Batch up to 96 inputs per request to lower per-item cost.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Global support search with mixed-language corpora.</li>
          <li>RAG pipelines feeding Claude Sonnet responses.</li>
          <li>Content recommendation and deduplication systems.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Normalize text (lowercase, trim whitespace) before embedding.</li>
          <li>Persist vectors in Postgres + pgvector for efficient retrieval.</li>
          <li>Cache frequently queried embeddings to avoid reprocessing.</li>
        </ul>
        <Callout title="Model ID">
          `cohere-embed-multilingual`
        </Callout>
        <Callout title="Input types">
          `search_document`, `search_query`, `classification`, `clustering`
        </Callout>
      </div>
    </Columns>
  </Accordion>
</AccordionGroup>
</section>

## Image models

Generate photorealistic and stylized visuals straight from prompts.

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 3rem', padding: '0 min(8vw, 96px)' }}>
<AccordionGroup>
  <Accordion title="Stable Image Ultra">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Supports 16:9, 1:1, 21:9, 2:3, 3:2, 4:5, 5:4, 9:16, and 9:21 aspect ratios.</li>
          <li>Resolutions up to 1536×640 with prompt adherence tuned for enterprise brand safety.</li>
          <li>Negative prompts and seed control for reproducible iterations.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Marketing and creative production pipelines.</li>
          <li>Product visualization and storyboarding.</li>
          <li>Social content generation with tight turnaround times.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Start with draft-size renders to validate prompts before increasing resolution.</li>
          <li>Store seeds alongside prompts so teams can reproduce edits.</li>
          <li>Use negative prompts to block banned styles or elements.</li>
        </ul>
        <Callout title="Model ID">
          `stable-image-ultra`
        </Callout>
      </div>
    </Columns>
  </Accordion>
</AccordionGroup>
</section>

## Model selection playbooks

### Picking a default chat model

1. Prototype with **Claude 4.5 Haiku** for fast, cost-effective performance.
2. Upgrade to **Claude 4.5 Sonnet** for balanced performance on complex tasks.
3. Escalate to **Claude 4 Sonnet** for high-stakes workflows (compliance, financial analysis, multi-step coding).
4. Use **Nova Lite** or **Nova Pro** if you need tighter alignment with AWS governance.

### Cost and performance controls

- Stream completions to surface partial output without waiting for full responses.
- Cap `max_tokens` based on UI constraints to avoid runaway output cost.
- Use `temperature` ≤ 0.5 for deterministic system flows.
- Batch embedding requests and reuse embeddings for unchanged documents.
- Track spend per app in the Heroku Dashboard and set alerts for anomalies.

### Provisioning checklist

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 3rem', padding: '0 min(8vw, 96px)' }}>
<Steps>
  <Step title="Inspect availability">

  ```bash
  heroku ai:models:list
  ```
  </Step>
  <Step title="Create a managed resource">

  ```bash
  heroku ai:models:create claude-3-5-sonnet --app my-ai-app
  ```
  </Step>
  <Step title="Verify attachment">

  ```bash
  heroku ai:models:info --app my-ai-app
  ```
  </Step>
</Steps>
</section>

## Related resources

<section style={{ margin: '0 calc(-1 * min(8vw, 96px)) 3rem', padding: '0 min(8vw, 96px)' }}>
<CardGroup cols={2}>
  <Card title="Chat Completions API" href="/inference-api/chat-completions" icon="comments">
    Implement conversational workloads with streaming and tool calling.
  </Card>
  <Card title="Embeddings API" href="/inference-api/embeddings" icon="vector-square">
    Build RAG, semantic search, and clustering pipelines.
  </Card>
  <Card title="Image Generation API" href="/inference-api/images-generations" icon="image">
    Turn prompts into assets for marketing, product, and design.
  </Card>
  <Card title="AI Studio" href="/heroku-inference/ai-studio" icon="palette">
    Evaluate prompts, compare models, and share test links.
  </Card>
</CardGroup>
</section>
