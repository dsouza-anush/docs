---
title: "Models overview"
description: "Choose the right Heroku AI model using side-by-side comparisons, playbooks, and provisioning tips"
---

Heroku Managed Inference and Agents exposes leading chat, embedding, and generative image models behind OpenAI-compatible endpoints. Use this page to spot the best fit for your workload, understand trade-offs, and move from evaluation to production quickly.

## Featured models

<Columns cols={3}>
  <Card title="Claude 4 Sonnet" icon="sparkles" href="#chat-models">
    Premium reasoning model with extended thinking, multimodal inputs, and the highest quality responses for complex assistants.
  </Card>
  <Card title="Claude 3.5 Haiku" icon="bolt" href="#chat-models">
    Fastest and most cost-efficient Claude option—ideal for realtime agents, support chat, and broadcast-style use cases.
  </Card>
  <Card title="Stable Image Ultra" icon="image" href="#image-models">
    Enterprise-grade text-to-image model with high prompt adherence, adjustable aspect ratios, and reproducible seeds.
  </Card>
</Columns>

## Compare models at a glance

| Model | Category | Strength | Relative cost | Context window | Extended thinking | Vision | Link |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Claude 4 Sonnet | Chat | Deep reasoning with chain-of-thought | Premium | 200K tokens | ✓ | ✓ | [Pricing](/inference-api/pricing#model-pricing-overview) |
| Claude 3.7 Sonnet | Chat | High intelligence with extended thinking | High | 200K tokens | ✓ | ✓ | [Pricing](/inference-api/pricing#model-pricing-overview) |
| Claude 3.5 Sonnet | Chat | Balanced quality, speed, and cost | Balanced | 200K tokens | Optional | ✓ | [Pricing](/inference-api/pricing#model-pricing-overview) |
| Claude 3.5 Haiku | Chat | Lowest latency, great for scale | Low | 200K tokens | ✗ | ✗ | [Pricing](/inference-api/pricing#model-pricing-overview) |
| Claude 3.0 Haiku | Chat | Legacy low-cost option | Low | 200K tokens | ✗ | ✗ | [Pricing](/inference-api/pricing#model-pricing-overview) |
| Amazon Nova Pro | Chat | Enterprise-first generalist | High | Large | ✗ | ✗ | [Pricing](/inference-api/pricing#model-pricing-overview) |
| Amazon Nova Lite | Chat | Efficient conversational model | Low | Medium | ✗ | ✗ | [Pricing](/inference-api/pricing#model-pricing-overview) |
| OpenAI GPT OSS 120B | Chat | Open-weight experimentation | Self-managed | 128K tokens | ✗ | ✗ | [Pricing](/inference-api/pricing#model-pricing-overview) |
| Cohere Embed Multilingual | Embeddings | Multilingual semantic search | Per 1M tokens | — | — | — | [Pricing](/inference-api/pricing#model-pricing-overview) |
| Stable Image Ultra | Image | High fidelity image synthesis | Per image | — | — | — | [Pricing](/inference-api/pricing#model-pricing-overview) |

<Note>
  Cost guidance reflects relative position within the Heroku AI catalog. Check the pricing page for live token and image rates before deploying to production.
</Note>

## Chat models

Advanced language models for assistants, copilots, and automated workflows.

<AccordionGroup>
  <Accordion title="Claude 4 Sonnet (premium reasoning)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Extended thinking unlocks deeper multi-step reasoning.</li>
          <li>Multimodal: accepts images, PDFs, and mixed media.</li>
          <li>200K token window for large documents and code bases.</li>
          <li>Supports tool calling and structured outputs.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Strategic analysis and research copilots.</li>
          <li>Complex coding assistants and debugging flows.</li>
          <li>Document intelligence use cases requiring full fidelity.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Enable extended thinking only when needed to manage output spend.</li>
          <li>Use evaluation harnesses in AI Studio before promoting new prompts.</li>
          <li>Stream responses to keep latency manageable for end users.</li>
        </ul>
        <Callout title="Model ID">
          `claude-4-sonnet`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="Claude 3.7 Sonnet (high IQ, lower cost)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Extended thinking for difficult reasoning with reduced pricing.</li>
          <li>Vision support and 200K context window.</li>
          <li>Fast enough for most production assistants.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Technical documentation Q&A bots.</li>
          <li>Code review companions.</li>
          <li>Enterprise knowledge management workflows.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Instrument latency metrics; responses are slightly faster than Claude 4.</li>
          <li>Reserve extended thinking for escalations to balance cost.</li>
          <li>Attach the model to staging apps to validate prompts before rollout.</li>
        </ul>
        <Callout title="Model ID">
          `claude-3-7-sonnet`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="Claude 3.5 Sonnet (balanced default)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Strong reasoning with faster average latency than Claude 4.</li>
          <li>Optional extended thinking when prompts demand it.</li>
          <li>Vision for OCR, screenshot analysis, and product imagery.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>General-purpose product assistants.</li>
          <li>Code generation where turnaround speed matters.</li>
          <li>Marketing content with light creative editing.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Set `max_tokens` between 600–1200 to keep responses snappy.</li>
          <li>Toggle streaming for user-facing chat experiences.</li>
          <li>Use as the baseline model in evaluation suites.</li>
        </ul>
        <Callout title="Model ID">
          `claude-3-5-sonnet`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="Claude 3.5 Haiku (high-volume choice)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Lowest latency across the Claude family.</li>
          <li>Full 200K context window for broad prompts.</li>
          <li>Optimized for throughput and predictable billing.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Customer support automations.</li>
          <li>Moderation or classification pipelines.</li>
          <li>Broadcast messaging, notifications, and templated replies.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>No extended thinking—design prompts accordingly.</li>
          <li>Batch inference when possible to maximize concurrency.</li>
          <li>Monitor token mix; output remains capped at 4K tokens.</li>
        </ul>
        <Callout title="Model ID">
          `claude-3-5-haiku`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="Claude 3.0 Haiku (legacy, low cost)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Ultra-low latency for simple prompts.</li>
          <li>Minimal cost overhead for pilot projects.</li>
          <li>Compatible with existing Haiku workloads.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Lightweight Q&A and FAQ bots.</li>
          <li>Content moderation stubs.</li>
          <li>Legacy apps migrating from older Anthropic versions.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Plan migration to 3.5 Haiku for better quality.</li>
          <li>Limit prompts to short instructions to maintain output quality.</li>
          <li>Keep an eye on model deprecation timelines.</li>
        </ul>
        <Callout title="Model ID">
          `claude-3-haiku`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="Amazon Nova Pro (enterprise generalist)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Strong reasoning with AWS-native integrations.</li>
          <li>Extended context for multilingual and domain-heavy prompts.</li>
          <li>Governance aligned with AWS Bedrock controls.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Workloads already standardized on AWS security tooling.</li>
          <li>Enterprise knowledge bases requiring PII guardrails.</li>
          <li>Finance and regulated industry copilots.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Cross-check request volumes with AWS cost allocation tags.</li>
          <li>Rely on Bedrock safety settings exposed via model parameters.</li>
          <li>Coordinate upgrades with AWS release cadence.</li>
        </ul>
        <Callout title="Model ID">
          `amazon-nova-pro`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="Amazon Nova Lite (efficient chat)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Lower price point compared to Nova Pro.</li>
          <li>Optimized for general conversational tasks.</li>
          <li>Shorter latency with manageable quality trade-offs.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Customer service and triage bots.</li>
          <li>Internal productivity copilots.</li>
          <li>High-volume prompts within the AWS ecosystem.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Use shorter prompts to keep responses sharp.</li>
          <li>Pair with Claude models for fallback escalation flows.</li>
          <li>Benchmark throughput against Haiku to choose the lowest-cost option.</li>
        </ul>
        <Callout title="Model ID">
          `amazon-nova-lite`
        </Callout>
      </div>
    </Columns>
  </Accordion>

  <Accordion title="OpenAI GPT OSS 120B (open-weight sandbox)">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Open-source friendly architecture and weights.</li>
          <li>Supports custom fine-tuning and adapters.</li>
          <li>128K context window for broader experimentation.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Research teams evaluating open models.</li>
          <li>Hybrid setups mixing managed and self-hosted inference.</li>
          <li>Education and experimentation environments.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Expect higher latency compared to hosted Claude models.</li>
          <li>Budget for additional evaluation since quality varies by prompt.</li>
          <li>Track GPU usage via Heroku metrics to control costs.</li>
        </ul>
        <Callout title="Model ID">
          `openai-gpt-oss-120b`
        </Callout>
      </div>
    </Columns>
  </Accordion>
</AccordionGroup>

## Embedding models

Use embeddings for semantic search, classification, clustering, and retrieval-augmented generation.

<AccordionGroup>
  <Accordion title="Cohere Embed Multilingual">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>100+ language support with 1,024-dimension vectors.</li>
          <li>Optimized presets for search, classification, and clustering.</li>
          <li>Batch up to 96 inputs per request to lower per-item cost.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Global support search with mixed-language corpora.</li>
          <li>RAG pipelines feeding Claude Sonnet responses.</li>
          <li>Content recommendation and deduplication systems.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Normalize text (lowercase, trim whitespace) before embedding.</li>
          <li>Persist vectors in Postgres + pgvector for efficient retrieval.</li>
          <li>Cache frequently queried embeddings to avoid reprocessing.</li>
        </ul>
        <Callout title="Model ID">
          `cohere-embed-multilingual`
        </Callout>
        <Callout title="Input types">
          `search_document`, `search_query`, `classification`, `clustering`
        </Callout>
      </div>
    </Columns>
  </Accordion>
</AccordionGroup>

## Image models

Generate photorealistic and stylized visuals straight from prompts.

<AccordionGroup>
  <Accordion title="Stable Image Ultra">
    <Columns cols={2}>
      <div>
        <h4>Highlights</h4>
        <ul>
          <li>Supports 16:9, 1:1, 21:9, 2:3, 3:2, 4:5, 5:4, 9:16, and 9:21 aspect ratios.</li>
          <li>Resolutions up to 1536×640 with prompt adherence tuned for enterprise brand safety.</li>
          <li>Negative prompts and seed control for reproducible iterations.</li>
        </ul>
        <h4>Best for</h4>
        <ul>
          <li>Marketing and creative production pipelines.</li>
          <li>Product visualization and storyboarding.</li>
          <li>Social content generation with tight turnaround times.</li>
        </ul>
      </div>
      <div>
        <h4>Operational tips</h4>
        <ul>
          <li>Start with draft-size renders to validate prompts before increasing resolution.</li>
          <li>Store seeds alongside prompts so teams can reproduce edits.</li>
          <li>Use negative prompts to block banned styles or elements.</li>
        </ul>
        <Callout title="Model ID">
          `stable-image-ultra`
        </Callout>
      </div>
    </Columns>
  </Accordion>
</AccordionGroup>

## Model selection playbooks

### Picking a default chat model

1. Prototype with **Claude 3.5 Haiku** for the lowest latency and cost.
2. Upgrade to **Claude 3.5 Sonnet** when prompts demand more reasoning or image understanding.
3. Escalate to **Claude 4 Sonnet** for high-stakes workflows (compliance, financial analysis, multi-step coding).
4. Use **Nova Lite** or **Nova Pro** if you need tighter alignment with AWS governance.

### Cost and performance controls

- Stream completions to surface partial output without waiting for full responses.
- Cap `max_tokens` based on UI constraints to avoid runaway output cost.
- Use `temperature` ≤ 0.5 for deterministic system flows.
- Batch embedding requests and reuse embeddings for unchanged documents.
- Track spend per app in the Heroku Dashboard and set alerts for anomalies.

### Provisioning checklist

<Steps>
  <Step title="Inspect availability">

  ```bash
  heroku ai:models:list
  ```
  </Step>
  <Step title="Create a managed resource">

  ```bash
  heroku ai:models:create claude-3-5-sonnet --app my-ai-app
  ```
  </Step>
  <Step title="Verify attachment">

  ```bash
  heroku ai:models:info --app my-ai-app
  ```
  </Step>
</Steps>

## Related resources

<CardGroup cols={2}>
  <Card title="Chat Completions API" href="/inference-api/chat-completions" icon="comments">
    Implement conversational workloads with streaming and tool calling.
  </Card>
  <Card title="Embeddings API" href="/inference-api/embeddings" icon="vector-square">
    Build RAG, semantic search, and clustering pipelines.
  </Card>
  <Card title="Image Generation API" href="/inference-api/images-generations" icon="image">
    Turn prompts into assets for marketing, product, and design.
  </Card>
  <Card title="AI Studio" href="/heroku-inference/ai-studio" icon="palette">
    Evaluate prompts, compare models, and share test links.
  </Card>
</CardGroup>
