---
title: "Chat Completions"
openapi: "POST /v1/chat/completions"
---

The `/v1/chat/completions` endpoint generates conversational completions for a provided set of input messages. You can specify the model, adjust generation settings such as temperature, and optionally stream responses or enable tool calling.

<Note>
  View our [available chat models](https://devcenter.heroku.com/articles/heroku-inference-api-model-cards) to see which models support which features.
</Note>

## Base URL

```
https://us.inference.heroku.com
```

## Authentication

All requests must include an `Authorization` header with your Heroku Inference API key:

```bash
Authorization: Bearer YOUR_INFERENCE_KEY
```

You can get your API key from your Heroku app's `INFERENCE_KEY` config variable.

## Request Parameters

### model
**string** · required

Model ID to use for completion, typically the value of your `INFERENCE_MODEL_ID` config var.

Example: `"claude-4-sonnet"`, `"claude-3-5-haiku"`

### messages
**array** · required

Array of message objects representing the conversation history. Each message must have a `role` and `content`.

Supported roles: `system`, `user`, `assistant`, `tool`

<CodeGroup>
```json Example
[
  {
    "role": "system",
    "content": "You are a helpful assistant."
  },
  {
    "role": "user",
    "content": "What is Heroku?"
  }
]
```
</CodeGroup>

### max_completion_tokens
**integer** · optional

Maximum number of tokens the model can generate before stopping.

- Max value: `4096` for Haiku models
- Max value: `8192` for Sonnet models

### temperature
**float** · optional · default: `1.0`

Controls randomness of the response. Range: `0.0` to `1.0`

- Values closer to `0` make responses more focused and deterministic
- Values closer to `1.0` encourage more creative and diverse responses

### top_p
**float** · optional · default: `0.999`

Nucleus sampling threshold. Range: `0` to `1.0`. Specifies the cumulative probability of tokens to consider.

### stream
**boolean** · optional · default: `false`

Stream responses incrementally via server-sent events. Useful for chat interfaces and avoiding timeout errors.

### stop
**array of strings** · optional

List of strings that stop the model from generating further tokens if encountered in the response.

### tools
**array of objects** · optional

List of tools the model may call. See [Tool Use Guide](/tool-use/heroku-tools) for details.

<Accordion title="Tool Object Structure">
```json
{
  "type": "function",
  "function": {
    "name": "get_weather",
    "description": "Get current weather for a location",
    "parameters": {
      "type": "object",
      "properties": {
        "location": {
          "type": "string",
          "description": "City and state, e.g. Portland, OR"
        }
      },
      "required": ["location"]
    }
  }
}
```
</Accordion>

### tool_choice
**string or object** · optional · default: `"required"`

Controls how the model uses provided tools.

- `"none"` - Model will not call any tools
- `"auto"` - Model can call zero or more tools
- `"required"` - Model must call at least one tool

### extended_thinking
**object** · optional

Enable extended thinking for Claude 3.7 Sonnet and Claude 4 Sonnet only. Allows the model to use additional internal tokens for reasoning steps.

<Accordion title="Extended Thinking Object">
```json
{
  "enabled": true,
  "budget_tokens": 1024,
  "include_reasoning": true
}
```

**Fields:**
- `enabled` (boolean): Enable extended thinking
- `budget_tokens` (integer): Minimum 1024, maximum varies by model
- `include_reasoning` (boolean): Include reasoning trace in response
</Accordion>

<Warning>
Extended thinking is only supported for Claude 3.7 Sonnet and Claude 4 Sonnet. Requests with `extended_thinking` for other models will fail.
</Warning>

## Response

### id
**string**

Unique identifier for the chat completion.

### object
**string**

Always returns `"chat.completion"`.

### created
**integer**

Unix timestamp when the completion was created.

### model
**string**

Model ID used to generate the response.

### choices
**array**

Array containing the generated message (always length 1).

<Accordion title="Choice Object">
**index** (integer): Index of the choice (always 0)

**message** (object): Generated message content
- `role` (string): Always `"assistant"`
- `content` (string): Text content of the response
- `tool_calls` (array, optional): Tool calls requested by the model
- `reasoning` (object, optional): Reasoning trace if extended thinking enabled

**finish_reason** (string): Reason the model stopped
- `"stop"` - Natural stopping point
- `"length"` - Reached max tokens
- `"tool_calls"` - Made tool calls
</Accordion>

### usage
**object**

Token usage statistics.

- `prompt_tokens` (integer): Tokens in the input
- `completion_tokens` (integer): Tokens in the output
- `total_tokens` (integer): Total tokens used

## Examples

<CodeGroup>

```bash cURL
curl https://us.inference.heroku.com/v1/chat/completions \
  -H "Authorization: Bearer $INFERENCE_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-4-sonnet",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "What is Heroku?"
      }
    ],
    "max_completion_tokens": 1024,
    "temperature": 0.7
  }'
```

```python Python
import os
import requests

url = f"{os.getenv('INFERENCE_URL')}/v1/chat/completions"
headers = {
    "Authorization": f"Bearer {os.getenv('INFERENCE_KEY')}",
    "Content-Type": "application/json"
}

data = {
    "model": os.getenv("INFERENCE_MODEL_ID"),
    "messages": [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is Heroku?"}
    ],
    "max_completion_tokens": 1024,
    "temperature": 0.7
}

response = requests.post(url, headers=headers, json=data)
print(response.json())
```

```javascript JavaScript
const response = await fetch(
  `${process.env.INFERENCE_URL}/v1/chat/completions`,
  {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.INFERENCE_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: process.env.INFERENCE_MODEL_ID,
      messages: [
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'user', content: 'What is Heroku?' }
      ],
      max_completion_tokens: 1024,
      temperature: 0.7
    })
  }
);

const data = await response.json();
console.log(data);
```

```ruby Ruby
require 'net/http'
require 'json'

uri = URI("#{ENV['INFERENCE_URL']}/v1/chat/completions")
request = Net::HTTP::Post.new(uri)
request['Authorization'] = "Bearer #{ENV['INFERENCE_KEY']}"
request['Content-Type'] = 'application/json'

request.body = {
  model: ENV['INFERENCE_MODEL_ID'],
  messages: [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'What is Heroku?' }
  ],
  max_completion_tokens: 1024,
  temperature: 0.7
}.to_json

response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: true) do |http|
  http.request(request)
end

puts JSON.parse(response.body)
```

</CodeGroup>

### Response Example

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1745623456,
  "model": "claude-4-sonnet",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Heroku is a cloud platform as a service (PaaS) that enables developers to build, run, and operate applications entirely in the cloud. It supports multiple programming languages and provides tools for deployment, scaling, and management of applications."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 28,
    "completion_tokens": 45,
    "total_tokens": 73
  }
}
```

## Tool Calling Example

<CodeGroup>

```bash Request with Tools
curl https://us.inference.heroku.com/v1/chat/completions \
  -H "Authorization: Bearer $INFERENCE_KEY" \
  -H "Content-Type": "application/json" \
  -d '{
    "model": "claude-4-sonnet",
    "messages": [
      {"role": "user", "content": "What is the weather in Portland?"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "get_weather",
          "description": "Get current weather for a location",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "City and state, e.g. Portland, OR"
              }
            },
            "required": ["location"]
          }
        }
      }
    ],
    "tool_choice": "required"
  }'
```

```json Tool Call Response
{
  "id": "chatcmpl-def456",
  "object": "chat.completion",
  "created": 1745623500,
  "model": "claude-4-sonnet",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": null,
        "tool_calls": [
          {
            "id": "toolu_abc123",
            "type": "function",
            "function": {
              "name": "get_weather",
              "arguments": "{\"location\":\"Portland, OR\"}"
            }
          }
        ]
      },
      "finish_reason": "tool_calls"
    }
  ],
  "usage": {
    "prompt_tokens": 120,
    "completion_tokens": 25,
    "total_tokens": 145
  }
}
```

</CodeGroup>

## Streaming

Enable streaming to receive incremental responses as server-sent events (SSE):

```bash
curl https://us.inference.heroku.com/v1/chat/completions \
  -H "Authorization: Bearer $INFERENCE_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-4-sonnet",
    "messages": [{"role": "user", "content": "Tell me a story"}],
    "stream": true
  }'
```

Each chunk contains a delta of the completion:

```json
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1745623456,"model":"claude-4-sonnet","choices":[{"index":0,"delta":{"role":"assistant","content":"Once"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1745623456,"model":"claude-4-sonnet","choices":[{"index":0,"delta":{"content":" upon"},"finish_reason":null}]}

data: [DONE]
```

## Related Endpoints

<CardGroup cols={2}>
  <Card title="Agents Endpoint" href="/inference-api/agents-heroku" icon="robot">
    Automatically execute Heroku tools
  </Card>
  <Card title="Embeddings" href="/inference-api/embeddings" icon="vector-square">
    Generate text embeddings
  </Card>
  <Card title="Image Generation" href="/inference-api/images-generations" icon="image">
    Create images with AI
  </Card>
  <Card title="Tool Use Guide" href="/tool-use/heroku-tools" icon="wrench">
    Available Heroku tools
  </Card>
</CardGroup>
